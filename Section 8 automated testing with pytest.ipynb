{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing automated tests with pytest\n",
    "\n",
    "In this section we will go over writing automated tests using pytest which is a testing framework for python. \n",
    "\n",
    "## Learning outcomes\n",
    "\n",
    "You will learn about\n",
    "* Why you should write automated test\n",
    "* How to write and run easy tests for pytest'\n",
    "* Use numpy testing for \n",
    "* How to write multi-parameter tests\n",
    "\n",
    "\n",
    "## Required\n",
    "\n",
    "* pytest module\n",
    "\n",
    "## Further reading\n",
    "* [coderefinery automated testing](https://coderefinery.github.io/testing/)\n",
    "* [Automation testing with pytest](https://medium.com/tenable-techblog/automation-testing-with-pytest-444c8b34ead2)\n",
    "* [pytest documentation](https://docs.pytest.org/en/latest/)\n",
    "\n",
    "### Acknowledgements\n",
    "* Some of the material in these notes has been adapted from [coderefinery automated testing](https://coderefinery.github.io/testing/) licenced under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use automated testing\n",
    "\n",
    "As hardware engineers we are often rightfully concerned with accuracy, reproducability and calibration of our measurement equipment and when we test equipment we often compare against an expected response, performance ... However, often we take much less care about our software and numerical code and if it performs correctly and instead do basic ad-hoc testing. For example do you always confirm that a change you made to your code did not result in an unexpected side-effect?\n",
    "\n",
    "Two horror stories:\n",
    "* [A Scientistâ€™s Nightmare: Software Problem Leads to Five Retractions](https://science.sciencemag.org/content/314/5807/1856.summary)\n",
    "* [Researchers find bug in Python script may have affected hundreds of studies](https://arstechnica.com/information-technology/2019/10/chemists-discover-cross-platform-python-scripts-not-so-cross-platform/)\n",
    "\n",
    "Automatic testing allows you to:\n",
    "* ensure that expected functionality is preserved\n",
    "* verify that code is doing what it is supposed to do\n",
    "* easier refactoring of code\n",
    "* easier contributions from external developers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing concepts\n",
    "\n",
    "Imperfect tests that exists and are run are better than perfect tests that do not exist. \n",
    "\n",
    "When testing you should\n",
    "* test often\n",
    "* ideally test automatically (using continuous integration for example)\n",
    "* test with numerical accuracy in mind\n",
    "\n",
    "There are basically three types of tests. \n",
    "\n",
    "### Unit tests\n",
    "\n",
    "* Unit tests test a single unit, e.g. module or function\n",
    "* Provide documentation of capability of the function (some frameworks even integrate them into the documentation)\n",
    "\n",
    "### Integration tests\n",
    "\n",
    "Also called functionality tests \n",
    "* Verify that your modules are working well together\n",
    "* Test the functionality of your project, e.g. do your simulations get correct results for known cases\n",
    "\n",
    "### Regression tests\n",
    "\n",
    "* Test over different versions of the code base\n",
    "* Can for example test if performance remains the same or does not regress\n",
    "\n",
    "We will not discuss regression tests here and largely focus on unit tests. \n",
    "\n",
    "### Test frameworks\n",
    "\n",
    "There exists a number of testing frameworks for Python, many with different advantages and disadvanteges. The most common are:\n",
    "* [pytest](http://doc.pytest.org/)\n",
    "* [nose](http://nose.readthedocs.io/)\n",
    "* [doctest](https://docs.python.org/2/library/doctest.html)\n",
    "* [unittest](https://docs.python.org/2/library/unittest.html)\n",
    "\n",
    "We will focus on pytest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytest\n",
    "\n",
    "Pytest is easy to use and write test for. \n",
    "\n",
    "### Installation \n",
    "\n",
    "If you are using _anaconda_ you can install pytest via `conda` or the _anaconda navigator_. It is also contained in many linux distributions or alternatively it can be installed with `pip install pytest`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first test\n",
    "\n",
    "Let us write a function an test it's functionality. Here we are writing function and tests in the same file, in practical projects tests are typically stored in a separate directory (e.g. tests) and the functions and modules to be tested are imported. \n",
    "\n",
    "```python\n",
    "# contents of test_example1.py\n",
    "def fahrenheit2celcius(T_f):\n",
    "    \"\"\"\n",
    "    Convert temperature in Fahrenheit to Celcius\n",
    "    \"\"\"\n",
    "    T_c = (T_f - 32.) * (5/9.)\n",
    "    return T_c\n",
    "\n",
    "\n",
    "def test_fahrenheit2celcius():\n",
    "    T_c = fahrenheit2celcius(32.)\n",
    "    expected_result = 0.\n",
    "    assert T_c == expected_result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.3.2, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /home/jschrod/Work/Code/OFCShortCourse/Notebooks/Hands_on_Advanced\n",
      "plugins: xonsh-0.9.10\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_example1.py \u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.21s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_example1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have a function which converts temperature in Fahrenheit to temperature in Celcius. We test that the function works correctly by testing against a known result at 0 degress Celcius. \n",
    "\n",
    "_pytest_ will run all files of the form `test_*.py` or `*_test.py` in the current directory and \n",
    "* all `test` prefixed functions or methods inside these files \n",
    "* all `test` prefixed functions or methods inside `Test` prefixed test classes (without an __init__ method)\n",
    "\n",
    "You can review the test discovery conventions [here](http://doc.pytest.org/en/latest/goodpractices.html#test-discovery)\n",
    "\n",
    "Generally you should use assert in your tests as this will allow pytest to use advanced assertion introspection which will give you more information on the test failure. \n",
    "\n",
    "## Parametrizing\n",
    "\n",
    "Now in the above test we only test for a single result. We could write test for multiple known results, but that quickly becomes tedious. Fortunately pytest offers a way to avoid much of this boilerplate. \n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "# contents of test_example2.py\n",
    "def fahrenheit2celcius(T_f):\n",
    "    \"\"\"\n",
    "    Convert temperature in Fahrenheit to Celcius\n",
    "    \"\"\"\n",
    "    T_c = (T_f - 32.) * (5/9.)\n",
    "    return T_c\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"t\", [(32,0), (451, 232.778)])\n",
    "def test_fahrenheit2celcius(t):\n",
    "    T_c = fahrenheit2celcius(t[0])\n",
    "    expected_result = t[1]\n",
    "    assert T_c == expected_result\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.7.3, pytest-5.3.2, py-1.8.1, pluggy-0.13.1\r\n",
      "rootdir: /home/jschrod/Work/Code/OFCShortCourse/Notebooks/Hands_on_Advanced\r\n",
      "plugins: xonsh-0.9.10\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 2 items                                                              \u001b[0m\r\n",
      "\r\n",
      "test_example2.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                      [100%]\u001b[0m\r\n",
      "\r\n",
      "=================================== FAILURES ===================================\r\n",
      "\u001b[31m\u001b[1m_________________________ test_fahrenheit2celcius[t1] __________________________\u001b[0m\r\n",
      "\r\n",
      "t = (451, 232.778)\r\n",
      "\r\n",
      "\u001b[1m    @pytest.mark.parametrize(\"t\", [(32,0), (451, 232.778)])\u001b[0m\r\n",
      "\u001b[1m    def test_fahrenheit2celcius(t):\u001b[0m\r\n",
      "\u001b[1m        T_c = fahrenheit2celcius(t[0])\u001b[0m\r\n",
      "\u001b[1m        expected_result = t[1]\u001b[0m\r\n",
      "\u001b[1m>       assert T_c == expected_result\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE       assert 232.7777777777778 == 232.778\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtest_example2.py\u001b[0m:16: AssertionError\r\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.03s\u001b[0m\u001b[31m ==========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest test_example2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test iterated over all parameters, however it fails for the second parameter set due to numerical accuracies. \n",
    "\n",
    "### Numpy testing\n",
    "\n",
    "We could use introduce our own way of accounting for numerical accuracy and do somthing like:\n",
    "```python\n",
    "assert abs(T_c - expected_result) < 1e-5\n",
    "```\n",
    "Fortunately there is the numpy testing module that was specifically designed for testing numerical code with limited accuracty, so lets use that instead.\n",
    "```python\n",
    "import pytest\n",
    "import numpy.testing as npt\n",
    "\n",
    "# contents of test_example3.py\n",
    "def fahrenheit2celcius(T_f):\n",
    "    \"\"\"\n",
    "    Convert temperature in Fahrenheit to Celcius\n",
    "    \"\"\"\n",
    "    T_c = (T_f - 32.) * (5/9.)\n",
    "    return T_c\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"t\", [(32,0), (451, 232.778)])\n",
    "def test_fahrenheit2celcius(t):\n",
    "    T_c = fahrenheit2celcius(t[0])\n",
    "    expected_result = t[1]\n",
    "    npt.assert_almost_equal(T_c, expected_result, 3)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.3.2, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /home/jschrod/Work/Code/OFCShortCourse/Notebooks/Hands_on_Advanced\n",
      "plugins: xonsh-0.9.10\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_example3.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.20s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_example3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function assert_almost_equal in module numpy.testing._private.utils:\n",
      "\n",
      "assert_almost_equal(actual, desired, decimal=7, err_msg='', verbose=True)\n",
      "    Raises an AssertionError if two items are not equal up to desired\n",
      "    precision.\n",
      "    \n",
      "    .. note:: It is recommended to use one of `assert_allclose`,\n",
      "              `assert_array_almost_equal_nulp` or `assert_array_max_ulp`\n",
      "              instead of this function for more consistent floating point\n",
      "              comparisons.\n",
      "    \n",
      "    The test verifies that the elements of ``actual`` and ``desired`` satisfy.\n",
      "    \n",
      "        ``abs(desired-actual) < 1.5 * 10**(-decimal)``\n",
      "    \n",
      "    That is a looser test than originally documented, but agrees with what the\n",
      "    actual implementation in `assert_array_almost_equal` did up to rounding\n",
      "    vagaries. An exception is raised at conflicting values. For ndarrays this\n",
      "    delegates to assert_array_almost_equal\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    actual : array_like\n",
      "        The object to check.\n",
      "    desired : array_like\n",
      "        The expected object.\n",
      "    decimal : int, optional\n",
      "        Desired precision, default is 7.\n",
      "    err_msg : str, optional\n",
      "        The error message to be printed in case of failure.\n",
      "    verbose : bool, optional\n",
      "        If True, the conflicting values are appended to the error message.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    AssertionError\n",
      "      If actual and desired are not equal up to specified precision.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    assert_allclose: Compare two array_like objects for equality with desired\n",
      "                     relative and/or absolute precision.\n",
      "    assert_array_almost_equal_nulp, assert_array_max_ulp, assert_equal\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy.testing as npt\n",
      "    >>> npt.assert_almost_equal(2.3333333333333, 2.33333334)\n",
      "    >>> npt.assert_almost_equal(2.3333333333333, 2.33333334, decimal=10)\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    AssertionError:\n",
      "    Arrays are not almost equal to 10 decimals\n",
      "     ACTUAL: 2.3333333333333\n",
      "     DESIRED: 2.33333334\n",
      "    \n",
      "    >>> npt.assert_almost_equal(np.array([1.0,2.3333333333333]),\n",
      "    ...                         np.array([1.0,2.33333334]), decimal=9)\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    AssertionError:\n",
      "    Arrays are not almost equal to 9 decimals\n",
      "    Mismatch: 50%\n",
      "    Max absolute difference: 6.66669964e-09\n",
      "    Max relative difference: 2.85715698e-09\n",
      "     x: array([1.         , 2.333333333])\n",
      "     y: array([1.        , 2.33333334])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy.testing as npt\n",
    "help(npt.assert_almost_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy.testing` also contains test for equality of arrays testing for absolute or relative equality etc. We highly recommend to use it in your tests.\n",
    "\n",
    "## Testing for exceptions\n",
    "\n",
    "Sometimes it might be desirable to test that functions raise exceptions when given the wrong input, instead of continuing and possibly producing bogus results much further down the line (which is often much harder to debug). \n",
    "\n",
    "In our example it does not make sense that the input temperature is complex, but the conversion function would still calculate with a complex input which might have unexpected consequences further down the line. Let us raise and exception and test for it.\n",
    "\n",
    "```python\n",
    "!cat test_example4.py\n",
    "\n",
    "# contents of test_example4.py\n",
    "import pytest\n",
    "import numpy.testing as npt\n",
    "import numpy as np\n",
    "\n",
    "def fahrenheit2celcius(T_f):\n",
    "    \"\"\"\n",
    "    Convert temperature in Fahrenheit to Celcius\n",
    "    \"\"\"\n",
    "    if not np.isreal(T_f):\n",
    "        raise TypeError(\"Temperature needs to be a real value\")\n",
    "    T_c = (T_f - 32.) * (5/9.)\n",
    "    return T_c\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"t\", [(32,0), (451, 232.778)])\n",
    "def test_fahrenheit2celcius(t):\n",
    "    T_c = fahrenheit2celcius(t[0])\n",
    "    expected_result = t[1]\n",
    "    npt.assert_almost_equal(T_c, expected_result, 3)\n",
    "\n",
    "def test_fahrenheit2celcius_type():\n",
    "    with pytest.raises(TypeError):\n",
    "        fahrenheit2celcius(1+1j)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.3.2, py-1.8.1, pluggy-0.13.1\n",
      "benchmark: 3.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/jschrod/Work/Code/ofcshortcourse/Notebooks/Hands_on_Advanced\n",
      "plugins: aspectlib-1.4.2, benchmark-3.2.3\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "test_example4.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_example4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping tests\n",
    "\n",
    "Sometimes it is desirable to group tests that belong together. For example group all tests for a specific function. This can be done by placing all the tests into a class prefixed with `Test`. Here is an example:\n",
    "\n",
    "```python\n",
    "# contents of test_example5.py\n",
    "import pytest\n",
    "import numpy.testing as npt\n",
    "import numpy as np\n",
    "\n",
    "def celcius2fahrenheit(T_c):\n",
    "    \"\"\"\n",
    "    Convert temperature in Fahrenheit to Celcius\n",
    "    \"\"\"\n",
    "    if not np.isreal(T_c):\n",
    "        raise TypeError(\"Temperature needs to be a real value\")\n",
    "    T_f = 9/5* T_c  + 32.\n",
    "    return T_f\n",
    "\n",
    "def fahrenheit2celcius(T_f):\n",
    "    \"\"\"\n",
    "    Convert temperature in Fahrenheit to Celcius\n",
    "    \"\"\"\n",
    "    if not np.isreal(T_f):\n",
    "        raise TypeError(\"Temperature needs to be a real value\")\n",
    "    T_c = (T_f - 32.) * (5/9.)\n",
    "    return T_c\n",
    "\n",
    "class TestConversionf2c(object):\n",
    "    @pytest.mark.parametrize(\"t\", [(32,0), (451, 232.778)])\n",
    "    def test_value(self, t):\n",
    "        T_c = fahrenheit2celcius(t[0])\n",
    "        expected_result = t[1]\n",
    "        npt.assert_almost_equal(T_c, expected_result, 3)\n",
    "\n",
    "    def test_type():\n",
    "        with pytest.raises(TypeError):\n",
    "            fahrenheit2celcius(1+1j)\n",
    "\n",
    "\n",
    "class TestConversionc2f(object):\n",
    "    @pytest.mark.parametrize(\"t\", [(0, 32), ( 232.778, 451)])\n",
    "    def test_value(self, t):\n",
    "        T_c = fahrenheit2celcius(t[0])\n",
    "        expected_result = t[1]\n",
    "        npt.assert_almost_equal(T_c, expected_result, 3)\n",
    "\n",
    "    def test_type():\n",
    "        with pytest.raises(TypeError):\n",
    "            fahrenheit2celcius(1+1j)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.3.2, py-1.8.1, pluggy-0.13.1\n",
      "benchmark: 3.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/jschrod/Work/Code/ofcshortcourse/Notebooks/Hands_on_Advanced\n",
      "plugins: aspectlib-1.4.2, benchmark-3.2.3\n",
      "collected 6 items                                                              \u001b[0m\n",
      "\n",
      "test_example5.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_example5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying tests\n",
    "\n",
    "This has the advantage that we can run the tests of only one group (class), for example if we working on one function but don't want to rerun all tests in the file (e.g. because it takes to long). To run only the test in the Celcius to Fahrenheit conversion tests one would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.3.2, py-1.8.1, pluggy-0.13.1\n",
      "benchmark: 3.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/jschrod/Work/Code/ofcshortcourse/Notebooks/Hands_on_Advanced\n",
      "plugins: aspectlib-1.4.2, benchmark-3.2.3\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "test_example5.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_example5.py::TestConversionc2f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can similarly only run the test of one method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.3.2, py-1.8.1, pluggy-0.13.1\n",
      "benchmark: 3.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/jschrod/Work/Code/ofcshortcourse/Notebooks/Hands_on_Advanced\n",
      "plugins: aspectlib-1.4.2, benchmark-3.2.3\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_example5.py \u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_example5.py::TestConversionc2f::test_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex selections are also possible (see [pytest docs specifying test](https://docs.pytest.org/en/latest/usage.html#specifying-tests-selecting-tests) for more details). For example run all type tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.3.2, py-1.8.1, pluggy-0.13.1\n",
      "benchmark: 3.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/jschrod/Work/Code/ofcshortcourse/Notebooks/Hands_on_Advanced\n",
      "plugins: aspectlib-1.4.2, benchmark-3.2.3\n",
      "collected 6 items / 4 deselected / 2 selected                                  \u001b[0m\n",
      "\n",
      "test_example5.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m4 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_example5.py -k \"test_type\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration for automated testing\n",
    "\n",
    "We do not have the time to cover how to integrate testing with other systems to achieve fully automated testing, however it should be noted that testing develops its full power when it integrates with the build or version control system to create a fully automated test environment. Two possibilities are:\n",
    "\n",
    "* integrate testing with the build system (setuptools, disttools) to run tests every time a new release is made.\n",
    "* integrate with the hosted version control system to run tests every time changes are added to the master (or a release branch). This is typically achieved using so-called _continuous integration_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on hardware\n",
    "\n",
    "While we focused here on running tests for software packages, it is quite easy to extend this work with hardware. This would enable to straight forwardly test performance of measurements or devices. [Luceda Photonics](https://www.lucedaphotonics.com/en) have shown some interesting work on automated testing of integrated circuits from design and modelling to measurement and validation of fabricated devices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Hopefully we have given you a taste of how you can use automated testing to improve your wor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
